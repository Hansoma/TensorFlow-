# 神经网络优化算法

本节将更加具体地介绍如何通过**反向传播算法(backpropagation)**和**梯度下降算法(gradient decent)** 调整神经网络中参数的取值.

**梯度下降算法** 主要用于优化单个参数的取值.
**反向传播算法** 给出了一个高效的方式在所有的参数上使用梯度下降算法.

**反向传播算法** 是训练神经网络的核心算法.

**梯度下降算法** 是最常用的神经网络优化算法.

神经网络模型中参数优化的过程直接决定了模型的质量, 是使用神经网络时十分重要的一步.

神经网络的优化过程可以分为两个阶段:

1. 先通过前向传播算法计算得到预测值, 并将预测值和真实值做对比得出两者之间的差距.
2. 通过反向传播算法计算损失函数对每一个参数的梯度, 再根据梯度和学习率使用梯度下降算法更新每一个参数.

* ### 注意:

1. 梯度下降算法并不能保证得到全局最优解, 而可能只是得到局部最优解.

2. 梯度下降算法的另一个问题就是计算时间太长, 因此可以使用随机梯度下降算法加速训练过程.

    而在实际应用中一般采用这两种算法的折中--每次计算一小部分训练数据的算是函数, 这一小部分数据被称为一个batch. 
    * 通过矩阵运算, 每次在一个batch上优化神经网络参数并不会比优化单个参数慢多少.
    * 另一方面, 每次用一个batch可以大大减小收敛所需的迭代次数, 同时可以使收敛的结果更接近梯度下降的效果. 

在本书中, 神经网络的训练大致遵循以下过程:
```
import tensorflow as tf

"""
在实际应用中,常采用综合梯度下降算法和随机梯度下降算法的折中,
即每次计算一小部分数据.
这一小部分数据被成为batch
"""

batch_size = n

# 每次读取一小部分数据作为当前的训练数据来执行反向传播算法
x = tf.placeholder(tf.float32, shape=(batch_size, 2), name='x-input')
y_ = tf.placeholder(tf.float32, shape=(batch_size, 1), name='y-input')

# 定义神经网络结构和优化算法
loss = ...
train_step = tf.train.AdamOptimizer(0.001).minimize(loss)

# 训练神经网络
with tf.Session() as sess:
    # 参数初始化
    ...
    # 迭代的更新参数
    for i in range(STEPS):
        # 准备batch_size个训练数据. 一般将所有的训练数据随机打乱之后再选取可以
        # 得到最好的优化效果
        current_X, current_Y = ...
        sess.run(train_step, feed_dict={x: current_X, y: current_Y})
```