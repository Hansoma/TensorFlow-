# 变量管理

* ## 在之前的*mnist demo* 中, 我们定义了一个函数来计算前向传播的结果.
    但是如果神经网络的结构更复杂, 参数更多时, 就需要一个更好的办法来传递和管理参数. 

    *TensorFlow* 中提供了通过变量名称来创建或者获取一个变量的机制.

    通过这个机制, 可以在不同的函数中直接通过变量的名字来使用变量, 而不用通过参数的形式到处传递. 

    *TensorFlow* 中通过变量名来获取变量的机制主要是通过tf.Variable,tf.get_variable来创建或者获取变量, 当 *tf.get_variable* 用于创建变量时, 它和 tf.Variable 的功能基本是等价的. 

    下面给出这两种函数创建变量的示例:
    ```
    v = tf.get_variable("v", shape=[1], 
                    initialiazer=tf.constant_initializer(1.0))
    v = tf.Variable(tf.constant(1.0, shape=[1], name="v"))
    ```
    对于tf.get_variable函数, 变量名称是一个必填的参数, 
    对于tf.Variable函数, 变量名称是一个可选的参数.

* ## *TensorFlow* 中有一些常用的变量初始化函数, 详情见书本p108表格

在我们的代码中, 如果第一层的权重已经叫weights, 那么创建第二层神经网络时,如果参数名仍然叫weights, 则会触发变量重用的错误.

下面这段代码说明了如何通过tf.variable_scope函数控制 tf.get_variable 函数已经获取过的已经创建过的变量.

```
import tensorflow as tf

# 在名字为foo的命名空间内创建名字为v的变量.
with tf.variable_scope("foo"):
    v = tf.get_variable(
        'v', [1], initializer=tf.constant_initializer(1.0)
    )

# 因为在命名空间foo中已经存在名字为v的变量, 所以下面的代码会报错:
# Variable foo/v already exists, disallowed.

'''
with tf.variable_scope("foo"):
    v = tf.get_variable("v", [1])
'''

# 在生成上下文管理器时， 将参数reuse设置为True， 这样tf.get_variable函数将直接获取
# 已经声明的变量。

with tf.variable_scope("foo", reuse=True):
    v1 = tf.get_variable("v", [1])
    print(v == v1)

# 将参数reuse设置为True时， tf.variable_scope将只能获取已经创建过的变量，
# 以为在命名空间bar中还没有创建变量v， 所以下面这段代码会报错。

'''
with tf.variable_scope("bar", reuse = True):
    v = tf.get_variable("v", [1])
'''
```

* ### tf.variable_scope 函数也是可以嵌套的. 

tf.variable_scope 函数除了可以控制 tf.variable , 也提供了一个管理变量命名空间的方式, 一下代码展示了如何通过 tf.variable_scope 来管理变量名称. 

```
import tensorflow as tf

v1 = tf.get_variable("v1", [1])
print(v1.name) # 输出v1:0 表示了这个变量是生成变量这个运算的地一个结果

with tf.variable_scope("foo"):
    v2 = tf.get_variable("v", [1])
    print(v2.name) # 输出foo/v:0, tf通过/来分隔命名空间的名称和变量的名称

with tf.variable_scope("foo"):
    with tf.variable_scope("bar"):
        v3 = tf.get_variable("v", [1])
        print(v3.name) # 输出foo/bar/v:0

    v4 = tf.get_variable("v1", [1])
    print(v4.name) # 输出foo/v1:0 命名空间退出之后就不会再加前缀了.

# 创建一个名称为空的命名空间, 并设置reuse = True
with tf.variable_scope("", reuse=True):
    v5 = tf.get_variable("foo/bar/v", [1]) # 可以直接通过带命名空间的变量名
                                            # 来获取其他明明空间下的变量.
    print(v5 == v3)

    v6 = tf.get_variable("foo/v1", [1])
    print(v6 == v4)
```

* 下面可以对demo02定义前向传播算法的地方做一些改进.
```
"""使用tf.variable_scope函数优化"""
def inference_improved(input_tensor, reuse = False):
    # 定义第一层神经网络的变量和前向传播过程.
    with tf.variable_scope('layer1', reuse = reuse):
        # 根据传进来的reuse来判断是创建新变量还是使用已经创建好的,
        # 在第一次构造网络时需要创建新的变量, 以后每次调用这个函数
        # 都直接使用reuse = True就不需要每次将变量传进来了.
        weights = tf.get_variable("weights", [INPUT_NODE, LAYER1_NODE],
                                  initializer=tf.truncated_normal_initializer(stddev=0.1))
        biases = tf.get_variable("biases", [LAYER1_NODE],
                                 initializer=tf.constant_initializer(0.0))
        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)

    # 类似的定义第二层神经网络的变量和前向传播过程
    with tf.variable_scope('layer2', reuse=reuse):
        weights = tf.get_variable("weights", [LAYER1_NODE, OUTPUT_NODE],
                                  initializer=tf.truncated_normal_initializer(stddev=0.1))
        biases = tf.get_variable("biases", [OUTPUT_NODE],
                                 initializer=tf.constant_initializer(0.0))
        layer2 = tf.matmul(layer1, weights) + biases

    return layer2
```