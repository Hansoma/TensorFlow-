# 神经网络搭建的一般步骤

1. ## forward.py
    ### 前向传播过程, 搭建网络的结构
    ```
    def forward(x, regularizer):
        '''regularizer为正则化权重'''
        w = 
        b = 
        y = 
        return y

    def get_weight(shape, regularizer):
        w = tf.Variable( )
        tf.add_to_collection("losses",
        # 将每一个w的正则损失加到总损失losses中
        tf.cotrib.layers.l2_regularizer(regularizer)(w))
        return w
    
    def get_bias(shape):
        '''实际上, shape就是每一层中b的个数'''
        b = tf.Variable( )
        return b
    ```
2. ## backward.py
    ### 反向传播过程, 训练网络, 优化参数. 
    ```
    def backward():
        x = tf.placeholder()
        y_ = tf.placeholder()
        y = forward.forward(x, REGULARIZER)
        global_step = tf.Variable(0, trainable=False)
        loss = 
    ```
    loss可以使用: 均方误差, 自定义, 交叉熵
    * 均方误差:
        
        ```
        loss_mse = tf.reduce_mean(tf.square(y-y_))
        ```
    
    * 交叉熵:
        
        ```
        ce = tf.nn.sparse_softmax_cross_entropy_with_logits(
            logits = y, 
            labels = tf.argmax(y_, 1))
        loss_ce = tf.reduce_mean(ce)
        ```
    
    * 以上加入正则化之后:

        ```
        loss = (y与y_的差距) + tf.add_n(tf.get_collection("losses"))
        ```

    若再加上使用指数衰减学习率:
    ```
    learning_rate = tf.train.exponential_decay(
        LEARNING_RATE_BASE, 
        global_step,
        数据集总样本数/BATCH_SIZE,
        LEARNING_RATE_DECAY,
        staircase = True)
    ```

    接上面代码:
    ```
    def back_ward():
        ... ...
        loss = 
        # 训练过程:
        train_step = tf.train.GradientDescentOptimize(
            learning_rate).minimize(loss, global_step=global_step)
        # 加入滑动平均(注意这里的global_step和指数衰减学习率共用):
        ema = tf.train.ExponentialMovingAverage(
            MOVING_AVERAGE_DECAY, global_step)
        ema_op = ema.apply(tf.trainable_variables())
        with tf.control_dependencies([train_step, ema_op]):
            train_op = tf.no_op(name="train")
    ```
    最后:

    ```
    with tf.Session() as sess:
        init_op = tf.global_variables_initializer()
        sess.run(init_op)
        for i in range(STEPS):
            sess.run(train_step, feed_dict={x: , y_: })
            if i%轮数 == :
                print()
    
    if __name__ == "__main__":
        backward()
    ```

