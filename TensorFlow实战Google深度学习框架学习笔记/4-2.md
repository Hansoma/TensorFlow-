# 自定义损失函数

## 4.2.1 经典损失函数

这一节介绍 **分类问题** 和 **回归问题** 中使用的经典损失函数.

* 交叉熵: 交叉熵刻画了两个概率分布之间的距离, 它是**分类问题**中使用比较广的一种损失函数.

    交叉熵的实现代码在之前提到过,如下:
    ```
    cross_entropy = -tf.reduce_mean(
        y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))
    ```
>Tip1: tf.clip_by_value()可以将一个张量中的数值限制在一个范围之内.
>
>下面给出一个使用demo:
>```
>"""
>下面这段代码,运行之后,小于2.5的将被替换成2.5
>大于4.5的将被替换成4.5
>"""
>sess = tf.Session()
>v = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
>print(tf.clip_by_value(v, 2.5, 4.5).eval(session=sess))
>```

>Tip2: tf.log()可以对一个张量中的所有元素一次取对数
>
>下面给出一个示例demo:
>```
>v = tf.constant([1.0, 2.0, 3.0])
>print(tf.log(v).eval(session=sess))
>```

>Tip3: 在实现交叉熵的代码中直接将两个矩阵通过"*"相乘.
这个操作不是矩阵乘法,而是元素之间直接相乘.
矩阵的乘法通过tf.matmul函数完成.
>
>下面的代码给出两者的区别:
>
>```
>v1 = tf.constant([[1.0, 2.0], [3.0, 4.0]])
>v2 = tf.constant([[5.0, 6.0], [7.0, 8.0]])
>"""
>"*"完成的是两个矩阵对应位置上两个数的乘积
>tf.matmul完成的是正经的矩阵乘法
>"""
>print((v1*v2).eval(session=sess))
>print(tf.matmul(v1, v2).eval(session=sess))
>```

以下代码展示了 *tf.reduce_mean()* 的用法.
```
v = tf.constant([1.0, 2.0, 3.0], [4.0, 5.0, 6.0])
print(tf.reduce_mean(v).eval())
```
因为交叉熵一般会和 *softmax* 回归一起使用, 所以*TensorFlow*对这两个功能进行了封装, 提供了`tf.nn.softmax_cross_entropy_with_logits`函数.

下面给出示例用法:
```
# 其中y代表了神经网络的原始输出结果, y_代表了标准答案.
cross_entropy = tf.nn.softmax_cross_entropy_with_logits(
    labels=y_, logits=y)
```
**均方误差(MSE, mean squared error)** 是解决回归问题最常用的损失函数.
以下代码展示了如何通过 *TensorFlow* 实现均方误差损失函数:

```
mse = tf.reduce_mean(tf.square(y_ - y))
```

## 4.2.2 自定义损失函数

*TensorFlow* 不仅支持经典损失函数, 还可以优化任意的自定义损失函数.

假设有一种商品的成本是1元,利润是10元,
那么进行回归预测时,少预测一个就少挣10元,
而多预测一个才少挣1元,因此为了利润最大化,需要自定义损失函数.

下面这段代码,实现了这个损失函数:

```
loss = tf.reduce_sum(tf.where(tf.greater(v1, v2),
                    (v1 - v2) * a, (v2 - v1) * b))
```
*tf.greater()* 输入两个张量,此函数会比较两个张量中每一个元素的大小,并返回比较结果,
当两个张量的维度不一样时,会进行类似 *numpy* 的广播操作
 *tf.where()* 有三个参数:
第一个为选择条件根据,当选择条件为 *True* 时,函数会选择第二个参数中的值.
否则使用第三个参数中的值.
注意: *tf.where* 函数的判断和选择都是在元素级别进行.
下面demo完整展示 *tf.where* 函数和 *tf.greater* 函数的用法:

```
#coding=utf-8

"""
tf.where和tf.greater函数的用法
"""

import tensorflow as tf

v1 = tf.constant([1.0, 2.0, 3.0, 4.0])
v2 = tf.constant([4.0, 3.0, 2.0, 1.0])

sess = tf.InteractiveSession()

#输出结果FFTT,前一个是否大于后一个
print(tf.greater(v1, v2).eval())

#类似C中的a>b?a:b
#输出结果:[4. 3. 3. 4.]
print(tf.where(tf.greater(v1, v2), v1, v2).eval())

sess.close()
```

